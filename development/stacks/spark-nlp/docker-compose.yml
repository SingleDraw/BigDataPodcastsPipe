
x-spark-worker: &x-spark-worker
  image: violetvi/spark-nlp:1.0.0 
  environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g

      - S3_ENDPOINT_URL=http://host.docker.internal:8333
      - AWS_SECRET_ACCESS_KEY_FILE=/run/secrets/aws_secret_key
      - AWS_ACCESS_KEY_ID_FILE=/run/secrets/aws_access_key

      # Azure Blob Storage configuration
      - STORAGE_ACCOUNT=${AZURE_STORAGE_ACCOUNT_NAME}
      - STORAGE_KEY=${AZURE_STORAGE_ACCOUNT_KEY}
      - TZ=Europe/Warsaw
  secrets:
      - aws_access_key
      - aws_secret_key
  volumes:
      - ./sink:/app/output
      - ./source:/app/input
      - ./nlp_job.py:/app/nlp_job.py:ro
  depends_on:
      - spark-master




services:
  # DISTRIBUTED SPARK SETUP FOR NLP PROCESSING
  # Spark Master and Worker setup for distributed processing
  # This setup is for running Spark NLP tasks using Celery workers
  # It uses a custom Spark worker image that has Spark NLP installed
  # It is also used as a Celery worker which runs the Spark NLP tasks
  # thrugh a Python script `stage_nlp.py` calling Spark

  spark-master:
    image: violetvi/spark-nlp:1.0.0 
    environment:
      - TZ=Europe/Warsaw
    volumes:
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    ports:
      - "8080:8080"
      - "7077:7077"

  # spark-worker-1:
  #   <<: *x-spark-worker
  #   container_name: spark-worker-1

  spark-nlp-celery-worker:
    <<: *x-spark-worker
    # command: ["celery", "-A", "tasks", "worker", "--loglevel=info"]
    container_name: spark-nlp-worker
    command: >
      spark-submit
      /app/nlp_job.py --input /app/input/podcast_joe_segmented.json --output s3a://whisper/spark_parquet/



# volumes:
#   azurite_data:

secrets:
  aws_access_key:
    file: ./secrets/s3_access_key.txt
  aws_secret_key:
    file: ./secrets/s3_secret_key.txt



